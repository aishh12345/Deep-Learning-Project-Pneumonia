{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920d8e11-91fa-4589-81af-d470a35a31c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.11/site-packages (0.23.1)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.11/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.4.2)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.16.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/lib/python3.11/site-packages (from seaborn) (3.8.4)\n",
      "Requirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.11/site-packages (from scikit-image) (1.13.0)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.11/site-packages (from scikit-image) (3.0)\n",
      "Requirement already satisfied: pillow>=9.1 in /opt/conda/lib/python3.11/site-packages (from scikit-image) (9.3.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.11/site-packages (from scikit-image) (2.34.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.11/site-packages (from scikit-image) (2024.2.12)\n",
      "Requirement already satisfied: packaging>=21 in /opt/conda/lib/python3.11/site-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.11/site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas seaborn scikit-image opencv-python scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ad691e-55c7-4362-9891-276f6597ff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, Adagrad\n",
    "from torch.autograd import Variable\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c887b1c-ae3a-44fd-bb27-082f15e2fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([transforms.Resize((150,150)),\n",
    "                                  transforms.Grayscale(num_output_channels=1),\n",
    "                                  transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(), # chnages from 0-255 to 0-1, numpy to tensors\n",
    "                                  transforms.Normalize([0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5])\n",
    "                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20643a7a-bffc-4f5e-bb1a-f84912358bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  # Print the device being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad24e292-b7e1-4803-b93f-d1da07cbcd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader\n",
    "\n",
    "batchSize=64\n",
    "train_path = 'data/train'\n",
    "train_loader = DataLoader(torchvision.datasets.ImageFolder(\n",
    "                          train_path,\n",
    "                          transform=transformer),\n",
    "                          batch_size = batchSize,\n",
    "                          shuffle=True)\n",
    "val_path = 'data/val'\n",
    "val_loader = DataLoader(torchvision.datasets.ImageFolder(\n",
    "                          val_path,\n",
    "                          transform=transformer),\n",
    "                          batch_size = batchSize)\n",
    "\n",
    "test_path = 'data/test'\n",
    "test_loader = DataLoader(torchvision.datasets.ImageFolder(\n",
    "                          test_path,\n",
    "                          transform=transformer),\n",
    "                          batch_size = batchSize,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b61acddc-ac21-4d79-b8f2-6209e527f91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in the training dataset: 5217\n",
      "Total number of images in the test dataset: 624\n",
      "Total number of images in the validation dataset: 16\n",
      "5217\n"
     ]
    }
   ],
   "source": [
    "total_images_train = len(train_loader.dataset)\n",
    "\n",
    "print(\"Total number of images in the training dataset:\", total_images_train)\n",
    "\n",
    "total_images_test = len(test_loader.dataset)\n",
    "\n",
    "print(\"Total number of images in the test dataset:\", total_images_test)\n",
    "\n",
    "total_images_val = len(val_loader.dataset)\n",
    "\n",
    "print(\"Total number of images in the validation dataset:\", total_images_val)\n",
    "\n",
    "print(total_images_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4dec5d6c-135e-4d47-a811-8730280a54f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, img_channels, class_count):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        # 150×150\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #75×75\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #37×37\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #18×18\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #9×9\n",
    "            \n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #4×4\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512 * 4 * 4, 256), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, class_count)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass input through conv layers\n",
    "        x = self.conv_layers(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Pass input through fc layers\n",
    "        x = self.fc_layers(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d642770c-1f68-4d62-a75e-1d2c63bbbf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class CustomCNN(nn.Module):\n",
    "#     def __init__(self, img_channels, class_count):\n",
    "#         super(CustomCNN, self).__init__()\n",
    "#         # 150×150\n",
    "        \n",
    "#         # Convolutional layers\n",
    "#         self.conv_layers = nn.Sequential(\n",
    "#             nn.Conv2d(img_channels, 64, kernel_size=3, padding=1),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #75×75\n",
    "            \n",
    "#             nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #37×37\n",
    "            \n",
    "#             nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #18×18\n",
    "            \n",
    "#             nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #9×9\n",
    "            \n",
    "#             nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #4×4\n",
    "#         )\n",
    "        \n",
    "#         # Fully connected layers\n",
    "#         self.fc_layers = nn.Sequential(\n",
    "#             nn.Linear(512 * 4 * 4, 256),  # Adjust the linear layer input size according to your image input size\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Linear(256, 64),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Linear(64, class_count)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Pass input through conv layers\n",
    "#         x = self.conv_layers(x)\n",
    "        \n",
    "#         # Flatten\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        \n",
    "#         # Pass input through fc layers\n",
    "#         x = self.fc_layers(x)\n",
    "        \n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e372ed4e-6fa8-4117-aa59-a0ee8e2c0b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomCNN(1, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a1204bc-f786-47e8-8c35-038125d86bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer =Adam(model.parameters(),lr = 0.0001,weight_decay=0.0001)\n",
    "optimizer =Adagrad(model.parameters(),lr = 0.001)\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "# loss_function = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ada85a2b-3582-4ddc-b5df-db28a594e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_for_class_normal = 5291 / (1341 * 2)\n",
    "weight_for_class_pneumonia = 5291 / (3875 * 2)\n",
    "\n",
    "class_weights_tensor = torch.tensor([weight_for_class_normal, weight_for_class_pneumonia], dtype=torch.float32)\n",
    "class_weights_tensor = class_weights_tensor.to(device)\n",
    "\n",
    "\n",
    "# weighted loss function using to ensure that the model gives prority to the normal samples as well\n",
    "#although they are much lesser in quantity\n",
    "weighted_loss_function = nn.CrossEntropyLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "74a5f421-2db5-49d8-a496-967f9c6f623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs =15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d9aab6fa-f00a-44bf-b775-56ce22db3f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: tensor(0.6939) Train Accuracy: 0.548974506421315 Val Accuracy: 0.5\n",
      "Epoch: 1 Train Loss: tensor(0.6935) Train Accuracy: 0.5806018784742188 Val Accuracy: 0.5\n",
      "Epoch: 2 Train Loss: tensor(0.6935) Train Accuracy: 0.5746597661491278 Val Accuracy: 0.5\n",
      "Epoch: 3 Train Loss: tensor(0.6934) Train Accuracy: 0.646540157178455 Val Accuracy: 0.5\n",
      "Epoch: 4 Train Loss: tensor(0.6935) Train Accuracy: 0.5309564884032969 Val Accuracy: 0.5\n",
      "Epoch: 5 Train Loss: tensor(0.6935) Train Accuracy: 0.617979681809469 Val Accuracy: 0.5\n",
      "Epoch: 6 Train Loss: tensor(0.6934) Train Accuracy: 0.7230208932336591 Val Accuracy: 0.5\n",
      "Epoch: 7 Train Loss: tensor(0.6933) Train Accuracy: 0.6091623538432049 Val Accuracy: 0.5\n",
      "Epoch: 8 Train Loss: tensor(0.6933) Train Accuracy: 0.7057695993866206 Val Accuracy: 0.5\n",
      "Epoch: 9 Train Loss: tensor(0.6933) Train Accuracy: 0.726279470960322 Val Accuracy: 0.5\n",
      "Epoch: 10 Train Loss: tensor(0.6933) Train Accuracy: 0.5094882116158712 Val Accuracy: 0.5\n",
      "Epoch: 11 Train Loss: tensor(0.6933) Train Accuracy: 0.7272378761740463 Val Accuracy: 0.5\n",
      "Epoch: 12 Train Loss: tensor(0.6933) Train Accuracy: 0.6854514088556641 Val Accuracy: 0.5\n",
      "Epoch: 13 Train Loss: tensor(0.6932) Train Accuracy: 0.5742764040636381 Val Accuracy: 0.5\n",
      "Epoch: 14 Train Loss: tensor(0.6933) Train Accuracy: 0.7429557216791259 Val Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "best_accuracy=0.0\n",
    "iterations=[]\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "\n",
    "\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs=model(images)\n",
    "        #using cross entropy loss (can explore BCE also)\n",
    "        loss=weighted_loss_function(outputs,labels)\n",
    "        #to calculate the loss wrt teh parameters and start backpropagation\n",
    "        loss.backward()\n",
    "        #to update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "\n",
    "        # train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        train_accuracy += torch.sum(prediction == labels.data).item()\n",
    "\n",
    "    #print(train_accuracy)\n",
    "    train_accuracy=train_accuracy/len(train_loader.dataset)\n",
    "    train_loss=train_loss/len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    # Evaluation on testing dataset\n",
    "    model.eval()\n",
    "\n",
    "    val_accuracy=0.0\n",
    "    for i, (images,labels) in enumerate(val_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "\n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        val_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "\n",
    "    val_accuracy=val_accuracy/total_images_val\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "\n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Val Accuracy: '+str(val_accuracy))\n",
    "\n",
    "torch.save(model.state_dict(), 'paths/model_adagrad_001_64_sigmoid.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "351b875b-7f08-43b4-96ae-4e70b37ea2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(), 'paths/model_adagrad_001_64_relu.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003af984-3444-49b1-8d70-af9b715ebdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_accuracies)\n",
    "\n",
    "epochs = range(1, len(train_accuracies) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_accuracies, 'bo-', label='Training accuracy')\n",
    "plt.plot(epochs, test_accuracies, 'ro-', label='Testing accuracy')\n",
    "plt.title('Training and Testing accuracy across epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5fc096-6604-4a23-b229-fa7991d89009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
